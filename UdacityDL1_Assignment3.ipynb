{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t656Fu_8vOkT"
   },
   "source": [
    "# Udacity ud730 Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFJahFqnvVqr"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2p04w72ZauP"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fr8IA1Npvc2Q"
   },
   "source": [
    "## reloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "a2DljHdxZnHi",
    "outputId": "4084d5ed-85a7-442d-8283-860cdb6ab8d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "# reload data\n",
    "\n",
    "pickle_file = '/content/drive/My Drive/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "She0CO1HZ1oE",
    "outputId": "378e1218-49de-4c58-f61e-87b5501bc5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (18724, 784) (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "# reshape data - flatten matrices and 1-hot encode labels\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2o3kZu7KZ3X1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noMHMi_Jvgnp"
   },
   "source": [
    "## Problem1: add l2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3O1EP-MRB-Tf"
   },
   "source": [
    "#### normal gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "TSqE0vG_Z5X3",
    "outputId": "1b426b70-d023-4326-d78a-dac63431cbdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Initialized with l2_penalty:  0.2 *******\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 78.2%\n",
      "Test accuracy: 84.9%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.1 *******\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 80.7%\n",
      "Test accuracy: 87.3%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.05 *******\n",
      "Training accuracy: 82.6%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 88.0%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.01 *******\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 88.9%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.005 *******\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 88.4%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.001 *******\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 76.8%\n",
      "Test accuracy: 84.5%\n"
     ]
    }
   ],
   "source": [
    "# normal gradient descent\n",
    "train_subset = 10000\n",
    "\n",
    "for i, l2_penalty in enumerate([0.2,0.1,0.05,0.01,0.005,0.001]):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data:\n",
    "        tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "        tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "        # Variables:\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "        biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "        # Training computation:\n",
    "        logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + \\\n",
    "            l2_penalty*tf.nn.l2_loss(weights)\n",
    "\n",
    "        # Optimizer:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "        # Predictions:\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "    num_steps = 801\n",
    "\n",
    "    def accuracy(predictions, labels):\n",
    "        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "                / predictions.shape[0])\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"\\n******* Initialized with l2_penalty: \", l2_penalty, \"*******\")\n",
    "        for step in range(num_steps):\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, :]))\n",
    "        print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-SHcOopB75E"
   },
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "id": "V-CP6WmcZ8KQ",
    "outputId": "25f40e4b-c310-460c-f839-3b979443d07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Initialized with l2_penalty:  0.01 *******\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 87.9%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.005 *******\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 88.3%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.001 *******\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 88.6%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.0005 *******\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 88.7%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.0001 *******\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "# stochastic gradient descent:\n",
    "\n",
    "tf_train_dataset = tf.constant(train_dataset[:5000, :])\n",
    "tf_train_labels = tf.constant(train_labels[:5000])\n",
    "\n",
    "for i, l2_penalty in enumerate([0.01,0.005,0.001,0.0005, 0.0001]):\n",
    "    batch_size = 128\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data:\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                          shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "        # Variables:\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "        biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "        # Training computation:\n",
    "        logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + \\\n",
    "            l2_penalty*tf.nn.l2_loss(weights)\n",
    "    \n",
    "        # Optimizer:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss  +\n",
    "                                                      0.005*tf.nn.l2_loss(weights))\n",
    "    \n",
    "        # Predictions:\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "    num_steps = 3001\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"\\n******* Initialized with l2_penalty: \", l2_penalty, \"*******\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            #if (step % 500 == 0):\n",
    "            #  print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNqQPaxoB1fq"
   },
   "source": [
    "#### 1relu layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "oset_AnHZ-ly",
    "outputId": "15ba67b0-3ef8-4eb6-a6e4-450eef7b4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Initialized with l2_penalty:  0.01 *******\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.7%\n",
      "Test accuracy: 90.2%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.005 *******\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.2%\n",
      "Test accuracy: 91.4%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.001 *******\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 92.8%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.0005 *******\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.2%\n",
      "Test accuracy: 91.0%\n"
     ]
    }
   ],
   "source": [
    "# SGD with 1-hidden layer relu (1024 nodes)\n",
    "\n",
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    "\n",
    "for i, l2_penalty in enumerate([0.01,0.005,0.001,0.0005]):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data:\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                          shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "        # Variables:\n",
    "        weights1 = tf.Variable(\n",
    "            tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "        biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "        weights2 = tf.Variable(\n",
    "          tf.truncated_normal([hidden_size, num_labels]))\n",
    "        biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "        # Training computation:\n",
    "        hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "        logits = tf.matmul(hidden, weights2) + biases2\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "              labels=tf_train_labels, logits=logits)) + \\\n",
    "              l2_penalty * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    \n",
    "        # Optimizer:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "        # Predictions:\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n",
    "        test_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)\n",
    "\n",
    "    num_steps = 3001\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"\\n******* Initialized with l2_penalty: \", l2_penalty, \"*******\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            #if (step % 500 == 0):\n",
    "            #  print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqJP7DYevrVb"
   },
   "source": [
    "## Problem2: Introduce overfit and see how l2 regularization affects it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1U3SPQOmBoLY"
   },
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "id": "wWt_3aegaArz",
    "outputId": "e4f3cd3e-a616-4cd8-bdfd-fb5449700e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Initialized with l2_penalty:  0 *******\n",
      "Minibatch accuracy: 98.0%\n",
      "Validation accuracy: 78.1%\n",
      "Test accuracy: 84.4%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.05 *******\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 78.6%\n",
      "Test accuracy: 85.3%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.01 *******\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 78.8%\n",
      "Test accuracy: 85.3%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.005 *******\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 78.5%\n",
      "Test accuracy: 85.0%\n",
      "\n",
      "******* Initialized with l2_penalty:  0.001 *******\n",
      "Minibatch accuracy: 98.0%\n",
      "Validation accuracy: 78.2%\n",
      "Test accuracy: 84.6%\n"
     ]
    }
   ],
   "source": [
    "# stochastic gradient descent:\n",
    "\n",
    "train_subdataset = train_dataset[:1000,:]\n",
    "train_sublabels = train_labels[:1000,:]\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "\n",
    "for i, l2_penalty in enumerate([0,0.05,0.01,0.005,0.001]):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data:\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                          shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "        # Variables:\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "        biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "        # Training computation:\n",
    "        logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + \\\n",
    "            l2_penalty*tf.nn.l2_loss(weights)\n",
    "    \n",
    "        # Optimizer:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss  +\n",
    "                                                      0.005*tf.nn.l2_loss(weights))\n",
    "    \n",
    "        # Predictions:\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "    num_steps = 3001\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"\\n******* Initialized with l2_penalty: \", l2_penalty, \"*******\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_sublabels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_subdataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_sublabels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            #if (step % 500 == 0):\n",
    "                #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k6_R4cWBBq9s"
   },
   "source": [
    "#### 1relu layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "id": "EphlNWfXaCer",
    "outputId": "bdd90754-21c2-4b41-c91d-e99877ac0051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Initialized with l2_penalty:  0 *******\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Test accuracy: 83.1%\n",
      "\n",
      "****** Initialized with l2_penalty:  0.05 *******\n",
      "Minibatch accuracy: 89.0%\n",
      "Validation accuracy: 79.3%\n",
      "Test accuracy: 86.5%\n",
      "\n",
      "****** Initialized with l2_penalty:  0.01 *******\n",
      "Minibatch accuracy: 99.5%\n",
      "Validation accuracy: 80.0%\n",
      "Test accuracy: 87.0%\n",
      "\n",
      "****** Initialized with l2_penalty:  0.005 *******\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 80.0%\n",
      "Test accuracy: 87.0%\n",
      "\n",
      "****** Initialized with l2_penalty:  0.001 *******\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.3%\n",
      "Test accuracy: 83.4%\n"
     ]
    }
   ],
   "source": [
    "# SGD with 1-hidden layer relu (1024 nodes):\n",
    "\n",
    "train_subdataset = train_dataset[:1000,:]\n",
    "train_sublabels = train_labels[:1000,:]\n",
    "\n",
    "batch_size = 200\n",
    "hidden_size = 1024\n",
    "\n",
    "for i, l2_penalty in enumerate([0,0.05,0.01,0.005,0.001]):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data:\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                          shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "        # Variables:\n",
    "        weights1 = tf.Variable(\n",
    "            tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "        biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "        weights2 = tf.Variable(\n",
    "          tf.truncated_normal([hidden_size, num_labels]))\n",
    "        biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "        # Training computation:\n",
    "        hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "        logits = tf.matmul(hidden, weights2) + biases2\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "              labels=tf_train_labels, logits=logits)) + \\\n",
    "              l2_penalty * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    \n",
    "        # Optimizer:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "        # Predictions:\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n",
    "        test_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)\n",
    "\n",
    "    num_steps = 3001\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"\\n****** Initialized with l2_penalty: \", l2_penalty, \"*******\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_sublabels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_subdataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_sublabels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            #if (step % 500 == 0):\n",
    "                #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtyBqPitv9C2"
   },
   "source": [
    "## Problem3: Add Dropout and see how it affects overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "1pS_MIwqaExN",
    "outputId": "b1ddede8-1e95-47b9-e271-ac10588c2171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Initialized with Dropout Probality:  1e-05 *******\n",
      "Minibatch accuracy: 8.0%\n",
      "Validation accuracy: 10.0%\n",
      "Test accuracy: 10.0%\n",
      "\n",
      "******* Initialized with Dropout Probality:  0.1 *******\n",
      "Minibatch accuracy: 76.0%\n",
      "Validation accuracy: 76.4%\n",
      "Test accuracy: 83.3%\n",
      "\n",
      "******* Initialized with Dropout Probality:  0.2 *******\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 80.3%\n",
      "Test accuracy: 87.3%\n",
      "\n",
      "******* Initialized with Dropout Probality:  0.3 *******\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.9%\n",
      "Test accuracy: 86.6%\n",
      "\n",
      "******* Initialized with Dropout Probality:  0.4 *******\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.8%\n",
      "Test accuracy: 86.9%\n",
      "\n",
      "******* Initialized with Dropout Probality:  0.5 *******\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Test accuracy: 86.6%\n",
      "\n",
      "******* Initialized with Dropout Probality:  0.6 *******\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Test accuracy: 85.8%\n"
     ]
    }
   ],
   "source": [
    "# SGD with 1-hidden layer relu (1024 nodes):\n",
    "\n",
    "train_subdataset = train_dataset[:1000,:]\n",
    "train_sublabels = train_labels[:1000,:]\n",
    "\n",
    "batch_size = 200\n",
    "hidden_size = 1024\n",
    "\n",
    "for i, keep_prob in enumerate([0.00001,0.1,0.2,0.3,0.4,0.5,0.6]):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data:\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                          shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "        # Variables:\n",
    "        weights1 = tf.Variable(\n",
    "            tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "        biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "        weights2 = tf.Variable(\n",
    "          tf.truncated_normal([hidden_size, num_labels]))\n",
    "        biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "        # Training computation:\n",
    "        hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1), keep_prob)\n",
    "        logits = tf.matmul(hidden, weights2) + biases2\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "              labels=tf_train_labels, logits=logits)) + \\\n",
    "              l2_penalty * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    \n",
    "        # Optimizer:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "        # Predictions:\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n",
    "        test_prediction = tf.nn.softmax(\n",
    "            tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)\n",
    "\n",
    "    num_steps = 3001\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"\\n******* Initialized with Dropout Probability: \", keep_prob, \"*******\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_sublabels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_subdataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_sublabels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            #if (step % 500 == 0):\n",
    "                #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DegZ20REwIiA"
   },
   "source": [
    "## Problem4: Try to get best accuracy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "FbO1NfZBaG0O",
    "outputId": "56941a20-6987-43a6-f475-8ee569f7eb52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 784) (10000, 784) (18724, 784)\n",
      "\n",
      "******* No Dropout in hidden Layers *******\n",
      "Epochs: 8 , Batch Size: 32 , Steps: 50000\n",
      "******* Initialized with l2_weight:  0 *******\n",
      "\n",
      " >>> Shuffled Data for Epoch: 1\n",
      "Minibatch loss at step 0: 2.296051\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 18.4%\n",
      "\n",
      " >>> Shuffled Data for Epoch: 2\n",
      "Minibatch loss at step 6250: 0.497033\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.1%\n",
      "\n",
      " >>> Shuffled Data for Epoch: 3\n",
      "Minibatch loss at step 12500: 0.045050\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "\n",
      " >>> Shuffled Data for Epoch: 4\n",
      "Minibatch loss at step 18750: 0.156950\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "\n",
      " >>> Shuffled Data for Epoch: 5\n",
      "Minibatch loss at step 25000: 0.208027\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "\n",
      " >>> Shuffled Data for Epoch: 6\n",
      "Minibatch loss at step 31250: 0.025898\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.1%\n",
      "\n",
      " >>> Shuffled Data for Epoch: 7\n",
      "Minibatch loss at step 37500: 0.085400\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.4%\n",
      "\n",
      " >>> Shuffled Data for Epoch: 8\n",
      "Minibatch loss at step 43750: 0.000569\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.5%\n",
      " ~~ Test accuracy: 96.5% ~~\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape, valid_dataset.shape, test_dataset.shape)\n",
    "\n",
    "# 5 Layers with with and without different Dropout and l2 regularizations\n",
    "\n",
    "# Constants:\n",
    "batch_size = 32\n",
    "hidden1_size = 1568\n",
    "hidden2_size = 784\n",
    "hidden3_size = 392\n",
    "hidden4_size = 196\n",
    "hidden5_size = 98\n",
    "            \n",
    "for j, keep_prob in enumerate([0]): # dropout search space\n",
    "    for i, l2_penalty in enumerate([0]): # l2 regularization search space\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "\n",
    "            # Input data:\n",
    "            tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "            tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "            tf_valid_dataset = tf.constant(valid_dataset)\n",
    "            tf_test_dataset = tf.constant(test_dataset)\n",
    "            \n",
    "            # Step count:\n",
    "            global_step = tf.Variable(0)\n",
    "            \n",
    "            # Weights and Biases:\n",
    "           \n",
    "            # HE init (better for relu?)\n",
    "            weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden1_size],\n",
    "                                                      stddev=np.sqrt(2.0/(image_size*image_size))))           \n",
    "            weights2 = tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size],\n",
    "                                                      stddev=np.sqrt(2.0/(hidden1_size))))            \n",
    "            weights3 = tf.Variable(tf.truncated_normal([hidden2_size, hidden3_size],\n",
    "                                                      stddev=np.sqrt(2.0/(hidden2_size)))) \n",
    "            weights4 = tf.Variable(tf.truncated_normal([hidden3_size, hidden4_size],\n",
    "                                                      stddev=np.sqrt(2.0/(hidden3_size))))            \n",
    "            weights5 = tf.Variable(tf.truncated_normal([hidden4_size, hidden5_size],\n",
    "                                                      stddev=np.sqrt(2.0/(hidden4_size))))\n",
    "            weights_out = tf.Variable(tf.truncated_normal([hidden5_size, num_labels],\n",
    "                                                      stddev=np.sqrt(2.0/(hidden5_size))))\n",
    "            \n",
    "            biases1 = tf.Variable(tf.zeros([hidden1_size]))\n",
    "            biases2 = tf.Variable(tf.zeros([hidden2_size]))\n",
    "            biases3 = tf.Variable(tf.zeros([hidden3_size]))\n",
    "            biases4 = tf.Variable(tf.zeros([hidden4_size]))\n",
    "            biases5 = tf.Variable(tf.zeros([hidden5_size]))\n",
    "            biases_out = tf.Variable(tf.zeros([num_labels])) \n",
    "            \n",
    "            ## xavier initializer\n",
    "            #initializer = tf.contrib.layers.xavier_initializer()\n",
    "            ## xavier init (better for sigmoid?)\n",
    "            #weights1 = tf.Variable(initializer([image_size * image_size, hidden1_size]))            \n",
    "            #weights2 = tf.Variable(initializer([hidden1_size, hidden2_size]))                       \n",
    "            #weights3 = tf.Variable(initializer([hidden2_size, hidden3_size]))                     \n",
    "            #weights4 = tf.Variable(initializer([hidden3_size, hidden4_size]))           \n",
    "            #weights5 = tf.Variable(initializer([hidden4_size, hidden5_size]))          \n",
    "            #weights_out = tf.Variable(initializer([hidden5_size, num_labels]))\n",
    "            #\n",
    "            #biases1 = tf.Variable(initializer([hidden1_size]))\n",
    "            #biases2 = tf.Variable(initializer([hidden2_size]))\n",
    "            #biases3 = tf.Variable(initializer([hidden3_size]))\n",
    "            #biases4 = tf.Variable(initializer([hidden4_size]))\n",
    "            #biases5 = tf.Variable(initializer([hidden5_size]))\n",
    "            #biases_out = tf.Variable(initializer([num_labels])) \n",
    "           \n",
    "            # Training:\n",
    "            if keep_prob != 0:\n",
    "                print('\\n*******Dropout keep Probability:', keep_prob,'*******')\n",
    "                hidden1 = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1), keep_prob)\n",
    "                hidden2 = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden1, weights2) + biases2), keep_prob)\n",
    "                hidden3 = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden2, weights3) + biases3), keep_prob)\n",
    "                hidden4 = tf.nn.relu(tf.matmul(hidden3, weights4) + biases4)\n",
    "                hidden5 = tf.nn.relu(tf.matmul(hidden4, weights5) + biases5)\n",
    "            else:\n",
    "                print('\\n******* No Dropout in hidden Layers','*******')\n",
    "                hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "                hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "                hidden3 = tf.nn.relu(tf.matmul(hidden2, weights3) + biases3)\n",
    "                hidden4 = tf.nn.relu(tf.matmul(hidden3, weights4) + biases4)\n",
    "                hidden5 = tf.nn.relu(tf.matmul(hidden4, weights5) + biases5)\n",
    "            output = tf.matmul(hidden5, weights_out) + biases_out\n",
    "            \n",
    "\n",
    "            # Loss Function & Regularization\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                                     labels=tf_train_labels, logits=output)) + \\\n",
    "                   l2_penalty * (\n",
    "                                    tf.nn.l2_loss(weights1) +\n",
    "                                    tf.nn.l2_loss(weights2) +\n",
    "                                    tf.nn.l2_loss(weights3) +\n",
    "                                    tf.nn.l2_loss(weights4) +\n",
    "                                    tf.nn.l2_loss(weights5) +\n",
    "                                    tf.nn.l2_loss(weights_out))\n",
    "\n",
    "            # Optimizer:\n",
    "            learning_rate = tf.train.exponential_decay(0.001, global_step, 5000, 0.7)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "            #optimizer = tf.train.AdamOptimizer(0.0008, 0.9, 0.999, 1e-07, False).minimize(loss)\n",
    "\n",
    "            # Train Prediction:\n",
    "            train_prediction = tf.nn.softmax(output)\n",
    "            \n",
    "            # Validation Prediction:\n",
    "            hidden1_v = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "            hidden2_v = tf.nn.relu(tf.matmul(hidden1_v, weights2) + biases2)\n",
    "            hidden3_v = tf.nn.relu(tf.matmul(hidden2_v, weights3) + biases3)\n",
    "            hidden4_v = tf.nn.relu(tf.matmul(hidden3_v, weights4) + biases4)\n",
    "            hidden5_v = tf.nn.relu(tf.matmul(hidden4_v, weights5) + biases5)\n",
    "            output_v = tf.matmul(hidden5_v, weights_out) + biases_out\n",
    "            valid_prediction = tf.nn.softmax(output_v)\n",
    "\n",
    "            # Test Prediction:\n",
    "            hidden1_t = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "            hidden2_t = tf.nn.relu(tf.matmul(hidden1_t, weights2) + biases2)\n",
    "            hidden3_t = tf.nn.relu(tf.matmul(hidden2_t, weights3) + biases3)\n",
    "            hidden4_t = tf.nn.relu(tf.matmul(hidden3_t, weights4) + biases4)\n",
    "            hidden5_t = tf.nn.relu(tf.matmul(hidden4_t, weights5) + biases5)\n",
    "            output_t = tf.matmul(hidden5_t, weights_out) + biases_out\n",
    "            test_prediction = tf.nn.softmax(output_t)\n",
    "\n",
    "        epochs = 8\n",
    "        num_steps = epochs * math.floor(train_dataset.shape[0] / batch_size)\n",
    "        print('Epochs:',epochs, ', Batch Size:', batch_size, ', Steps:', num_steps)\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            \n",
    "            print(\"******* Initialized with l2_weight: \", l2_penalty, \"*******\")\n",
    "            tf.global_variables_initializer().run()\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "                \n",
    "                # shuffle after each epoch\n",
    "                if step % math.floor(train_dataset.shape[0] / batch_size) == 0: \n",
    "                    perm = list(np.random.permutation(train_dataset.shape[0]))\n",
    "                    train_dataset = train_dataset[perm, :]\n",
    "                    train_labels = train_labels[perm, :]\n",
    "                    print('\\n >>> Shuffled Data for Epoch:' ,1+ math.ceil(step/math.floor(train_dataset.shape[0] / batch_size)))\n",
    "                \n",
    "                # create minibatches\n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                \n",
    "                # run graph\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "                _, l, predictions = session.run(\n",
    "                    [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "                if (step % math.floor(train_dataset.shape[0] / batch_size) == 0):\n",
    "                    print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                    print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                    print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print(\" ~~ Test accuracy: %.1f%% ~~\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lOYNUYbcDjqr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "UdacityDL1_Assignment3",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

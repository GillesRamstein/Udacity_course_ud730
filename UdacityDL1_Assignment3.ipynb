{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UdacityDL1_Assignment3",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "t656Fu_8vOkT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Udacity ud730 Assignment 3"
      ]
    },
    {
      "metadata": {
        "id": "rFJahFqnvVqr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports and loading file from Drive"
      ]
    },
    {
      "metadata": {
        "id": "T2p04w72ZauP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from six.moves import cPickle as pickle\n",
        "from six.moves import range\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q422X-KRuEbG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c0e598d3-4d14-42be-f3f4-a36c1823248c"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "51KMMbj8ujFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1decdbc5-eb71-4d5e-f131-f8d688f017db"
      },
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/Colab Data\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bewerbung.gdoc\t\t\t\t  Links.gdoc\n",
            "CMAS Lab\t\t\t\t  notMNIST.pickle\n",
            "Colab Notebooks\t\t\t\t  PolyLabLetter.gdoc\n",
            "Copy of DS 2018 07 CV Template.gslides\t  ToolKursStruktur\n",
            "CV.gdoc\t\t\t\t\t  Untitled document (1).gdoc\n",
            "flowers-recognition.zip\t\t\t  Untitled document.gdoc\n",
            "Gilles Ramstein         Ettingen 13.gdoc  Untitled spreadsheet (1).gsheet\n",
            "Gilles To-Do.gdoc\t\t\t  Untitled spreadsheet (2).gsheet\n",
            "Global Warming (Recovered).gsheet\t  Untitled spreadsheet.gsheet\n",
            "krankenkassen rechner.gsheet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fr8IA1Npvc2Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## reloading Data"
      ]
    },
    {
      "metadata": {
        "id": "a2DljHdxZnHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4084d5ed-85a7-442d-8283-860cdb6ab8d5"
      },
      "cell_type": "code",
      "source": [
        "# reload data\n",
        "\n",
        "pickle_file = '/content/drive/My Drive/notMNIST.pickle'\n",
        "\n",
        "with open(pickle_file, 'rb') as f:\n",
        "    save = pickle.load(f)\n",
        "    train_dataset = save['train_dataset']\n",
        "    train_labels = save['train_labels']\n",
        "    valid_dataset = save['valid_dataset']\n",
        "    valid_labels = save['valid_labels']\n",
        "    test_dataset = save['test_dataset']\n",
        "    test_labels = save['test_labels']\n",
        "    del save  # hint to help gc free up memory\n",
        "    print('Training set', train_dataset.shape, train_labels.shape)\n",
        "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "    print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 28, 28) (200000,)\n",
            "Validation set (10000, 28, 28) (10000,)\n",
            "Test set (18724, 28, 28) (18724,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "She0CO1HZ1oE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "378e1218-49de-4c58-f61e-87b5501bc5bb"
      },
      "cell_type": "code",
      "source": [
        "# reshape data - flatten matrices and 1-hot encode labels\n",
        "\n",
        "image_size = 28\n",
        "num_labels = 10\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
        "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
        "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "    return dataset, labels\n",
        "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
        "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
        "print('Training set', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 784) (200000, 10)\n",
            "Validation set (10000, 784) (10000, 10)\n",
            "Test set (18724, 784) (18724, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2o3kZu7KZ3X1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(predictions, labels):\n",
        "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "noMHMi_Jvgnp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem1: add l2 regularization"
      ]
    },
    {
      "metadata": {
        "id": "3O1EP-MRB-Tf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### normal gradient descent"
      ]
    },
    {
      "metadata": {
        "id": "TSqE0vG_Z5X3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "1b426b70-d023-4326-d78a-dac63431cbdc"
      },
      "cell_type": "code",
      "source": [
        "# normal gradient descent\n",
        "train_subset = 10000\n",
        "\n",
        "for i, l2_penalty in enumerate([0.2,0.1,0.05,0.01,0.005,0.001]):\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "\n",
        "        # Input data:\n",
        "        tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
        "        tf_train_labels = tf.constant(train_labels[:train_subset])\n",
        "        tf_valid_dataset = tf.constant(valid_dataset)\n",
        "        tf_test_dataset = tf.constant(test_dataset)\n",
        "    \n",
        "        # Variables:\n",
        "        weights = tf.Variable(\n",
        "            tf.truncated_normal([image_size * image_size, num_labels]))\n",
        "        biases = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "        # Training computation:\n",
        "        logits = tf.matmul(tf_train_dataset, weights) + biases\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + \\\n",
        "            l2_penalty*tf.nn.l2_loss(weights)\n",
        "\n",
        "        # Optimizer:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "    \n",
        "        # Predictions:\n",
        "        train_prediction = tf.nn.softmax(logits)\n",
        "        valid_prediction = tf.nn.softmax(\n",
        "            tf.matmul(tf_valid_dataset, weights) + biases)\n",
        "        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
        "\n",
        "    num_steps = 801\n",
        "\n",
        "    def accuracy(predictions, labels):\n",
        "        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "                / predictions.shape[0])\n",
        "\n",
        "    with tf.Session(graph=graph) as session:\n",
        "        tf.global_variables_initializer().run()\n",
        "        print(\"\\n******* Initialized with l2_penalty: \", l2_penalty, \"*******\")\n",
        "        for step in range(num_steps):\n",
        "            _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
        "        print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, :]))\n",
        "        print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
        "        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "******* Initialized with l2_penalty:  0.2 *******\n",
            "Training accuracy: 79.4%\n",
            "Validation accuracy: 78.2%\n",
            "Test accuracy: 84.9%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.1 *******\n",
            "Training accuracy: 81.7%\n",
            "Validation accuracy: 80.7%\n",
            "Test accuracy: 87.3%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.05 *******\n",
            "Training accuracy: 82.6%\n",
            "Validation accuracy: 81.4%\n",
            "Test accuracy: 88.0%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.01 *******\n",
            "Training accuracy: 84.4%\n",
            "Validation accuracy: 82.2%\n",
            "Test accuracy: 88.9%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.005 *******\n",
            "Training accuracy: 84.6%\n",
            "Validation accuracy: 81.7%\n",
            "Test accuracy: 88.4%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.001 *******\n",
            "Training accuracy: 80.8%\n",
            "Validation accuracy: 76.8%\n",
            "Test accuracy: 84.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X-SHcOopB75E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### SGD"
      ]
    },
    {
      "metadata": {
        "id": "V-CP6WmcZ8KQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "25f40e4b-c310-460c-f839-3b979443d07a"
      },
      "cell_type": "code",
      "source": [
        "# stochastic gradient descent:\n",
        "\n",
        "tf_train_dataset = tf.constant(train_dataset[:5000, :])\n",
        "tf_train_labels = tf.constant(train_labels[:5000])\n",
        "\n",
        "for i, l2_penalty in enumerate([0.01,0.005,0.001,0.0005, 0.0001]):\n",
        "    batch_size = 128\n",
        "\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "\n",
        "        # Input data:\n",
        "        tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                          shape=(batch_size, image_size * image_size))\n",
        "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "        tf_valid_dataset = tf.constant(valid_dataset)\n",
        "        tf_test_dataset = tf.constant(test_dataset)\n",
        "    \n",
        "        # Variables:\n",
        "        weights = tf.Variable(\n",
        "            tf.truncated_normal([image_size * image_size, num_labels]))\n",
        "        biases = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "        # Training computation:\n",
        "        logits = tf.matmul(tf_train_dataset, weights) + biases\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + \\\n",
        "            l2_penalty*tf.nn.l2_loss(weights)\n",
        "    \n",
        "        # Optimizer:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss  +\n",
        "                                                      0.005*tf.nn.l2_loss(weights))\n",
        "    \n",
        "        # Predictions:\n",
        "        train_prediction = tf.nn.softmax(logits)\n",
        "        valid_prediction = tf.nn.softmax(\n",
        "            tf.matmul(tf_valid_dataset, weights) + biases)\n",
        "        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
        "\n",
        "    num_steps = 3001\n",
        "\n",
        "    with tf.Session(graph=graph) as session:\n",
        "        tf.global_variables_initializer().run()\n",
        "        print(\"\\n******* Initialized with l2_penalty: \", l2_penalty, \"*******\")\n",
        "        for step in range(num_steps):\n",
        "            # Pick an offset within the training data, which has been randomized.\n",
        "            # Note: we could use better randomization across epochs.\n",
        "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "            # Generate a minibatch.\n",
        "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "            # and the value is the numpy array to feed to it.\n",
        "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "            _, l, predictions = session.run(\n",
        "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "            #if (step % 500 == 0):\n",
        "            #  print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
        "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "******* Initialized with l2_penalty:  0.01 *******\n",
            "Minibatch accuracy: 80.5%\n",
            "Validation accuracy: 81.5%\n",
            "Test accuracy: 87.9%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.005 *******\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 81.9%\n",
            "Test accuracy: 88.3%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.001 *******\n",
            "Minibatch accuracy: 82.0%\n",
            "Validation accuracy: 82.1%\n",
            "Test accuracy: 88.6%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.0005 *******\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 82.1%\n",
            "Test accuracy: 88.7%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.0001 *******\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 82.2%\n",
            "Test accuracy: 88.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aNqQPaxoB1fq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1relu layer"
      ]
    },
    {
      "metadata": {
        "id": "oset_AnHZ-ly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "15ba67b0-3ef8-4eb6-a6e4-450eef7b4459"
      },
      "cell_type": "code",
      "source": [
        "# SGD with 1-hidden layer relu (1024 nodes)\n",
        "\n",
        "batch_size = 128\n",
        "hidden_size = 1024\n",
        "\n",
        "for i, l2_penalty in enumerate([0.01,0.005,0.001,0.0005]):\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "\n",
        "        # Input data:\n",
        "        tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                          shape=(batch_size, image_size * image_size))\n",
        "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "        tf_valid_dataset = tf.constant(valid_dataset)\n",
        "        tf_test_dataset = tf.constant(test_dataset)\n",
        "    \n",
        "        # Variables:\n",
        "        weights1 = tf.Variable(\n",
        "            tf.truncated_normal([image_size * image_size, hidden_size]))\n",
        "        biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
        "        weights2 = tf.Variable(\n",
        "          tf.truncated_normal([hidden_size, num_labels]))\n",
        "        biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "        # Training computation:\n",
        "        hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "        logits = tf.matmul(hidden, weights2) + biases2\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "              labels=tf_train_labels, logits=logits)) + \\\n",
        "              l2_penalty * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
        "    \n",
        "        # Optimizer:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "    \n",
        "        # Predictions:\n",
        "        train_prediction = tf.nn.softmax(logits)\n",
        "        valid_prediction = tf.nn.softmax(\n",
        "            tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n",
        "        test_prediction = tf.nn.softmax(\n",
        "            tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)\n",
        "\n",
        "    num_steps = 3001\n",
        "\n",
        "    with tf.Session(graph=graph) as session:\n",
        "        tf.global_variables_initializer().run()\n",
        "        print(\"\\n******* Initialized with l2_penalty: \", l2_penalty, \"*******\")\n",
        "        for step in range(num_steps):\n",
        "            # Pick an offset within the training data, which has been randomized.\n",
        "            # Note: we could use better randomization across epochs.\n",
        "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "            # Generate a minibatch.\n",
        "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "            # and the value is the numpy array to feed to it.\n",
        "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "            _, l, predictions = session.run(\n",
        "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "            #if (step % 500 == 0):\n",
        "            #  print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
        "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "******* Initialized with l2_penalty:  0.01 *******\n",
            "Minibatch accuracy: 86.7%\n",
            "Validation accuracy: 83.7%\n",
            "Test accuracy: 90.2%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.005 *******\n",
            "Minibatch accuracy: 89.8%\n",
            "Validation accuracy: 85.2%\n",
            "Test accuracy: 91.4%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.001 *******\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 86.4%\n",
            "Test accuracy: 92.8%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.0005 *******\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 84.2%\n",
            "Test accuracy: 91.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iqJP7DYevrVb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem2: Introduce overfit and see how l2 regularization affects it"
      ]
    },
    {
      "metadata": {
        "id": "1U3SPQOmBoLY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### SGD"
      ]
    },
    {
      "metadata": {
        "id": "wWt_3aegaArz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "e4f3cd3e-a616-4cd8-bdfd-fb5449700e5e"
      },
      "cell_type": "code",
      "source": [
        "# stochastic gradient descent:\n",
        "\n",
        "train_subdataset = train_dataset[:1000,:]\n",
        "train_sublabels = train_labels[:1000,:]\n",
        "\n",
        "batch_size = 200\n",
        "\n",
        "\n",
        "for i, l2_penalty in enumerate([0,0.05,0.01,0.005,0.001]):\n",
        "\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "\n",
        "        # Input data:\n",
        "        tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                          shape=(batch_size, image_size * image_size))\n",
        "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "        tf_valid_dataset = tf.constant(valid_dataset)\n",
        "        tf_test_dataset = tf.constant(test_dataset)\n",
        "    \n",
        "        # Variables:\n",
        "        weights = tf.Variable(\n",
        "            tf.truncated_normal([image_size * image_size, num_labels]))\n",
        "        biases = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "        # Training computation:\n",
        "        logits = tf.matmul(tf_train_dataset, weights) + biases\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits)) + \\\n",
        "            l2_penalty*tf.nn.l2_loss(weights)\n",
        "    \n",
        "        # Optimizer:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss  +\n",
        "                                                      0.005*tf.nn.l2_loss(weights))\n",
        "    \n",
        "        # Predictions:\n",
        "        train_prediction = tf.nn.softmax(logits)\n",
        "        valid_prediction = tf.nn.softmax(\n",
        "            tf.matmul(tf_valid_dataset, weights) + biases)\n",
        "        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
        "\n",
        "    num_steps = 3001\n",
        "\n",
        "    with tf.Session(graph=graph) as session:\n",
        "        tf.global_variables_initializer().run()\n",
        "        print(\"\\n******* Initialized with l2_penalty: \", l2_penalty, \"*******\")\n",
        "        for step in range(num_steps):\n",
        "            # Pick an offset within the training data, which has been randomized.\n",
        "            # Note: we could use better randomization across epochs.\n",
        "            offset = (step * batch_size) % (train_sublabels.shape[0] - batch_size)\n",
        "            # Generate a minibatch.\n",
        "            batch_data = train_subdataset[offset:(offset + batch_size), :]\n",
        "            batch_labels = train_sublabels[offset:(offset + batch_size), :]\n",
        "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "            # and the value is the numpy array to feed to it.\n",
        "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "            _, l, predictions = session.run(\n",
        "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "            #if (step % 500 == 0):\n",
        "                #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
        "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "******* Initialized with l2_penalty:  0 *******\n",
            "Minibatch accuracy: 98.0%\n",
            "Validation accuracy: 78.1%\n",
            "Test accuracy: 84.4%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.05 *******\n",
            "Minibatch accuracy: 85.0%\n",
            "Validation accuracy: 78.6%\n",
            "Test accuracy: 85.3%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.01 *******\n",
            "Minibatch accuracy: 94.5%\n",
            "Validation accuracy: 78.8%\n",
            "Test accuracy: 85.3%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.005 *******\n",
            "Minibatch accuracy: 96.5%\n",
            "Validation accuracy: 78.5%\n",
            "Test accuracy: 85.0%\n",
            "\n",
            "******* Initialized with l2_penalty:  0.001 *******\n",
            "Minibatch accuracy: 98.0%\n",
            "Validation accuracy: 78.2%\n",
            "Test accuracy: 84.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k6_R4cWBBq9s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1relu layer\n"
      ]
    },
    {
      "metadata": {
        "id": "EphlNWfXaCer",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "bdd90754-21c2-4b41-c91d-e99877ac0051"
      },
      "cell_type": "code",
      "source": [
        "# SGD with 1-hidden layer relu (1024 nodes):\n",
        "\n",
        "train_subdataset = train_dataset[:1000,:]\n",
        "train_sublabels = train_labels[:1000,:]\n",
        "\n",
        "batch_size = 200\n",
        "hidden_size = 1024\n",
        "\n",
        "for i, l2_penalty in enumerate([0,0.05,0.01,0.005,0.001]):\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "\n",
        "        # Input data:\n",
        "        tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                          shape=(batch_size, image_size * image_size))\n",
        "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "        tf_valid_dataset = tf.constant(valid_dataset)\n",
        "        tf_test_dataset = tf.constant(test_dataset)\n",
        "    \n",
        "        # Variables:\n",
        "        weights1 = tf.Variable(\n",
        "            tf.truncated_normal([image_size * image_size, hidden_size]))\n",
        "        biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
        "        weights2 = tf.Variable(\n",
        "          tf.truncated_normal([hidden_size, num_labels]))\n",
        "        biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "        # Training computation:\n",
        "        hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "        logits = tf.matmul(hidden, weights2) + biases2\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "              labels=tf_train_labels, logits=logits)) + \\\n",
        "              l2_penalty * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
        "    \n",
        "        # Optimizer:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "    \n",
        "        # Predictions:\n",
        "        train_prediction = tf.nn.softmax(logits)\n",
        "        valid_prediction = tf.nn.softmax(\n",
        "            tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n",
        "        test_prediction = tf.nn.softmax(\n",
        "            tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)\n",
        "\n",
        "    num_steps = 3001\n",
        "\n",
        "    with tf.Session(graph=graph) as session:\n",
        "        tf.global_variables_initializer().run()\n",
        "        print(\"\\n****** Initialized with l2_penalty: \", l2_penalty, \"*******\")\n",
        "        for step in range(num_steps):\n",
        "            # Pick an offset within the training data, which has been randomized.\n",
        "            # Note: we could use better randomization across epochs.\n",
        "            offset = (step * batch_size) % (train_sublabels.shape[0] - batch_size)\n",
        "            # Generate a minibatch.\n",
        "            batch_data = train_subdataset[offset:(offset + batch_size), :]\n",
        "            batch_labels = train_sublabels[offset:(offset + batch_size), :]\n",
        "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "            # and the value is the numpy array to feed to it.\n",
        "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "            _, l, predictions = session.run(\n",
        "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "            #if (step % 500 == 0):\n",
        "                #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
        "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "****** Initialized with l2_penalty:  0 *******\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 76.2%\n",
            "Test accuracy: 83.1%\n",
            "\n",
            "****** Initialized with l2_penalty:  0.05 *******\n",
            "Minibatch accuracy: 89.0%\n",
            "Validation accuracy: 79.3%\n",
            "Test accuracy: 86.5%\n",
            "\n",
            "****** Initialized with l2_penalty:  0.01 *******\n",
            "Minibatch accuracy: 99.5%\n",
            "Validation accuracy: 80.0%\n",
            "Test accuracy: 87.0%\n",
            "\n",
            "****** Initialized with l2_penalty:  0.005 *******\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 80.0%\n",
            "Test accuracy: 87.0%\n",
            "\n",
            "****** Initialized with l2_penalty:  0.001 *******\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 76.3%\n",
            "Test accuracy: 83.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GtyBqPitv9C2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem3: Add Dropout and see how it affects overfitting"
      ]
    },
    {
      "metadata": {
        "id": "1pS_MIwqaExN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "b1ddede8-1e95-47b9-e271-ac10588c2171"
      },
      "cell_type": "code",
      "source": [
        "# SGD with 1-hidden layer relu (1024 nodes):\n",
        "\n",
        "train_subdataset = train_dataset[:1000,:]\n",
        "train_sublabels = train_labels[:1000,:]\n",
        "\n",
        "batch_size = 200\n",
        "hidden_size = 1024\n",
        "\n",
        "for i, keep_prob in enumerate([0.00001,0.1,0.2,0.3,0.4,0.5,0.6]):\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "\n",
        "        # Input data:\n",
        "        tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                          shape=(batch_size, image_size * image_size))\n",
        "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "        tf_valid_dataset = tf.constant(valid_dataset)\n",
        "        tf_test_dataset = tf.constant(test_dataset)\n",
        "    \n",
        "        # Variables:\n",
        "        weights1 = tf.Variable(\n",
        "            tf.truncated_normal([image_size * image_size, hidden_size]))\n",
        "        biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
        "        weights2 = tf.Variable(\n",
        "          tf.truncated_normal([hidden_size, num_labels]))\n",
        "        biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "        # Training computation:\n",
        "        hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1), keep_prob)\n",
        "        logits = tf.matmul(hidden, weights2) + biases2\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "              labels=tf_train_labels, logits=logits)) + \\\n",
        "              l2_penalty * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
        "    \n",
        "        # Optimizer:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "    \n",
        "        # Predictions:\n",
        "        train_prediction = tf.nn.softmax(logits)\n",
        "        valid_prediction = tf.nn.softmax(\n",
        "            tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n",
        "        test_prediction = tf.nn.softmax(\n",
        "            tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)\n",
        "\n",
        "    num_steps = 3001\n",
        "\n",
        "    with tf.Session(graph=graph) as session:\n",
        "        tf.global_variables_initializer().run()\n",
        "        print(\"\\n******* Initialized with Dropout Probability: \", keep_prob, \"*******\")\n",
        "        for step in range(num_steps):\n",
        "            # Pick an offset within the training data, which has been randomized.\n",
        "            # Note: we could use better randomization across epochs.\n",
        "            offset = (step * batch_size) % (train_sublabels.shape[0] - batch_size)\n",
        "            # Generate a minibatch.\n",
        "            batch_data = train_subdataset[offset:(offset + batch_size), :]\n",
        "            batch_labels = train_sublabels[offset:(offset + batch_size), :]\n",
        "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "            # and the value is the numpy array to feed to it.\n",
        "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "            _, l, predictions = session.run(\n",
        "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "            #if (step % 500 == 0):\n",
        "                #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "        print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
        "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "******* Initialized with Dropout Probality:  1e-05 *******\n",
            "Minibatch accuracy: 8.0%\n",
            "Validation accuracy: 10.0%\n",
            "Test accuracy: 10.0%\n",
            "\n",
            "******* Initialized with Dropout Probality:  0.1 *******\n",
            "Minibatch accuracy: 76.0%\n",
            "Validation accuracy: 76.4%\n",
            "Test accuracy: 83.3%\n",
            "\n",
            "******* Initialized with Dropout Probality:  0.2 *******\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 80.3%\n",
            "Test accuracy: 87.3%\n",
            "\n",
            "******* Initialized with Dropout Probality:  0.3 *******\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 79.9%\n",
            "Test accuracy: 86.6%\n",
            "\n",
            "******* Initialized with Dropout Probality:  0.4 *******\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 79.8%\n",
            "Test accuracy: 86.9%\n",
            "\n",
            "******* Initialized with Dropout Probality:  0.5 *******\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 79.5%\n",
            "Test accuracy: 86.6%\n",
            "\n",
            "******* Initialized with Dropout Probality:  0.6 *******\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 79.4%\n",
            "Test accuracy: 85.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DegZ20REwIiA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem4: Try to get best accuracy! "
      ]
    },
    {
      "metadata": {
        "id": "FbO1NfZBaG0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "56941a20-6987-43a6-f475-8ee569f7eb52"
      },
      "cell_type": "code",
      "source": [
        "print(train_dataset.shape, valid_dataset.shape, test_dataset.shape)\n",
        "\n",
        "# 5 Layers with with and without different Dropout and l2 regularizations\n",
        "\n",
        "# Constants:\n",
        "batch_size = 32\n",
        "hidden1_size = 1568\n",
        "hidden2_size = 784\n",
        "hidden3_size = 392\n",
        "hidden4_size = 196\n",
        "hidden5_size = 98\n",
        "            \n",
        "for j, keep_prob in enumerate([0]): # dropout search space\n",
        "    for i, l2_penalty in enumerate([0]): # l2 regularization search space\n",
        "        graph = tf.Graph()\n",
        "        with graph.as_default():\n",
        "\n",
        "            # Input data:\n",
        "            tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
        "            tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "            tf_valid_dataset = tf.constant(valid_dataset)\n",
        "            tf_test_dataset = tf.constant(test_dataset)\n",
        "            \n",
        "            # Step count:\n",
        "            global_step = tf.Variable(0)\n",
        "            \n",
        "            # Weights and Biases:\n",
        "           \n",
        "            # HE init (better for relu?)\n",
        "            weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden1_size],\n",
        "                                                      stddev=np.sqrt(2.0/(image_size*image_size))))           \n",
        "            weights2 = tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size],\n",
        "                                                      stddev=np.sqrt(2.0/(hidden1_size))))            \n",
        "            weights3 = tf.Variable(tf.truncated_normal([hidden2_size, hidden3_size],\n",
        "                                                      stddev=np.sqrt(2.0/(hidden2_size)))) \n",
        "            weights4 = tf.Variable(tf.truncated_normal([hidden3_size, hidden4_size],\n",
        "                                                      stddev=np.sqrt(2.0/(hidden3_size))))            \n",
        "            weights5 = tf.Variable(tf.truncated_normal([hidden4_size, hidden5_size],\n",
        "                                                      stddev=np.sqrt(2.0/(hidden4_size))))\n",
        "            weights_out = tf.Variable(tf.truncated_normal([hidden5_size, num_labels],\n",
        "                                                      stddev=np.sqrt(2.0/(hidden5_size))))\n",
        "            \n",
        "            biases1 = tf.Variable(tf.zeros([hidden1_size]))\n",
        "            biases2 = tf.Variable(tf.zeros([hidden2_size]))\n",
        "            biases3 = tf.Variable(tf.zeros([hidden3_size]))\n",
        "            biases4 = tf.Variable(tf.zeros([hidden4_size]))\n",
        "            biases5 = tf.Variable(tf.zeros([hidden5_size]))\n",
        "            biases_out = tf.Variable(tf.zeros([num_labels])) \n",
        "            \n",
        "            ## xavier initializer\n",
        "            #initializer = tf.contrib.layers.xavier_initializer()\n",
        "            ## xavier init (better for sigmoid?)\n",
        "            #weights1 = tf.Variable(initializer([image_size * image_size, hidden1_size]))            \n",
        "            #weights2 = tf.Variable(initializer([hidden1_size, hidden2_size]))                       \n",
        "            #weights3 = tf.Variable(initializer([hidden2_size, hidden3_size]))                     \n",
        "            #weights4 = tf.Variable(initializer([hidden3_size, hidden4_size]))           \n",
        "            #weights5 = tf.Variable(initializer([hidden4_size, hidden5_size]))          \n",
        "            #weights_out = tf.Variable(initializer([hidden5_size, num_labels]))\n",
        "            #\n",
        "            #biases1 = tf.Variable(initializer([hidden1_size]))\n",
        "            #biases2 = tf.Variable(initializer([hidden2_size]))\n",
        "            #biases3 = tf.Variable(initializer([hidden3_size]))\n",
        "            #biases4 = tf.Variable(initializer([hidden4_size]))\n",
        "            #biases5 = tf.Variable(initializer([hidden5_size]))\n",
        "            #biases_out = tf.Variable(initializer([num_labels])) \n",
        "           \n",
        "            # Training:\n",
        "            if keep_prob != 0:\n",
        "                print('\\n*******Dropout keep Probability:', keep_prob,'*******')\n",
        "                hidden1 = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1), keep_prob)\n",
        "                hidden2 = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden1, weights2) + biases2), keep_prob)\n",
        "                hidden3 = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden2, weights3) + biases3), keep_prob)\n",
        "                hidden4 = tf.nn.relu(tf.matmul(hidden3, weights4) + biases4)\n",
        "                hidden5 = tf.nn.relu(tf.matmul(hidden4, weights5) + biases5)\n",
        "            else:\n",
        "                print('\\n******* No Dropout in hidden Layers','*******')\n",
        "                hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "                hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
        "                hidden3 = tf.nn.relu(tf.matmul(hidden2, weights3) + biases3)\n",
        "                hidden4 = tf.nn.relu(tf.matmul(hidden3, weights4) + biases4)\n",
        "                hidden5 = tf.nn.relu(tf.matmul(hidden4, weights5) + biases5)\n",
        "            output = tf.matmul(hidden5, weights_out) + biases_out\n",
        "            \n",
        "\n",
        "            # Loss Function & Regularization\n",
        "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "                                     labels=tf_train_labels, logits=output)) + \\\n",
        "                   l2_penalty * (\n",
        "                                    tf.nn.l2_loss(weights1) +\n",
        "                                    tf.nn.l2_loss(weights2) +\n",
        "                                    tf.nn.l2_loss(weights3) +\n",
        "                                    tf.nn.l2_loss(weights4) +\n",
        "                                    tf.nn.l2_loss(weights5) +\n",
        "                                    tf.nn.l2_loss(weights_out))\n",
        "\n",
        "            # Optimizer:\n",
        "            learning_rate = tf.train.exponential_decay(0.001, global_step, 5000, 0.7)\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "            #optimizer = tf.train.AdamOptimizer(0.0008, 0.9, 0.999, 1e-07, False).minimize(loss)\n",
        "\n",
        "            # Train Prediction:\n",
        "            train_prediction = tf.nn.softmax(output)\n",
        "            \n",
        "            # Validation Prediction:\n",
        "            hidden1_v = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
        "            hidden2_v = tf.nn.relu(tf.matmul(hidden1_v, weights2) + biases2)\n",
        "            hidden3_v = tf.nn.relu(tf.matmul(hidden2_v, weights3) + biases3)\n",
        "            hidden4_v = tf.nn.relu(tf.matmul(hidden3_v, weights4) + biases4)\n",
        "            hidden5_v = tf.nn.relu(tf.matmul(hidden4_v, weights5) + biases5)\n",
        "            output_v = tf.matmul(hidden5_v, weights_out) + biases_out\n",
        "            valid_prediction = tf.nn.softmax(output_v)\n",
        "\n",
        "            # Test Prediction:\n",
        "            hidden1_t = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
        "            hidden2_t = tf.nn.relu(tf.matmul(hidden1_t, weights2) + biases2)\n",
        "            hidden3_t = tf.nn.relu(tf.matmul(hidden2_t, weights3) + biases3)\n",
        "            hidden4_t = tf.nn.relu(tf.matmul(hidden3_t, weights4) + biases4)\n",
        "            hidden5_t = tf.nn.relu(tf.matmul(hidden4_t, weights5) + biases5)\n",
        "            output_t = tf.matmul(hidden5_t, weights_out) + biases_out\n",
        "            test_prediction = tf.nn.softmax(output_t)\n",
        "\n",
        "        epochs = 8\n",
        "        num_steps = epochs * math.floor(train_dataset.shape[0] / batch_size)\n",
        "        print('Epochs:',epochs, ', Batch Size:', batch_size, ', Steps:', num_steps)\n",
        "        with tf.Session(graph=graph) as session:\n",
        "            \n",
        "            print(\"******* Initialized with l2_weight: \", l2_penalty, \"*******\")\n",
        "            tf.global_variables_initializer().run()\n",
        "            \n",
        "            for step in range(num_steps):\n",
        "                \n",
        "                # shuffle after each epoch\n",
        "                if step % math.floor(train_dataset.shape[0] / batch_size) == 0: \n",
        "                    perm = list(np.random.permutation(train_dataset.shape[0]))\n",
        "                    train_dataset = train_dataset[perm, :]\n",
        "                    train_labels = train_labels[perm, :]\n",
        "                    print('\\n >>> Shuffled Data for Epoch:' ,1+ math.ceil(step/math.floor(train_dataset.shape[0] / batch_size)))\n",
        "                \n",
        "                # create minibatches\n",
        "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "                batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "                \n",
        "                # run graph\n",
        "                feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "                _, l, predictions = session.run(\n",
        "                    [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "                if (step % math.floor(train_dataset.shape[0] / batch_size) == 0):\n",
        "                    print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "                    print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "                    print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
        "            print(\" ~~ Test accuracy: %.1f%% ~~\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200000, 784) (10000, 784) (18724, 784)\n",
            "\n",
            "******* No Dropout in hidden Layers *******\n",
            "Epochs: 8 , Batch Size: 32 , Steps: 50000\n",
            "******* Initialized with l2_weight:  0 *******\n",
            "\n",
            " >>> Shuffled Data for Epoch: 1\n",
            "Minibatch loss at step 0: 2.296051\n",
            "Minibatch accuracy: 15.6%\n",
            "Validation accuracy: 18.4%\n",
            "\n",
            " >>> Shuffled Data for Epoch: 2\n",
            "Minibatch loss at step 6250: 0.497033\n",
            "Minibatch accuracy: 84.4%\n",
            "Validation accuracy: 88.1%\n",
            "\n",
            " >>> Shuffled Data for Epoch: 3\n",
            "Minibatch loss at step 12500: 0.045050\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 89.6%\n",
            "\n",
            " >>> Shuffled Data for Epoch: 4\n",
            "Minibatch loss at step 18750: 0.156950\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 90.2%\n",
            "\n",
            " >>> Shuffled Data for Epoch: 5\n",
            "Minibatch loss at step 25000: 0.208027\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 90.8%\n",
            "\n",
            " >>> Shuffled Data for Epoch: 6\n",
            "Minibatch loss at step 31250: 0.025898\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 91.1%\n",
            "\n",
            " >>> Shuffled Data for Epoch: 7\n",
            "Minibatch loss at step 37500: 0.085400\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 91.4%\n",
            "\n",
            " >>> Shuffled Data for Epoch: 8\n",
            "Minibatch loss at step 43750: 0.000569\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 91.5%\n",
            " ~~ Test accuracy: 96.5% ~~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lOYNUYbcDjqr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}